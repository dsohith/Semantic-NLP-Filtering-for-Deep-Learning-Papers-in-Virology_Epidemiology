{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d621140",
   "metadata": {},
   "source": [
    "# Semantic NLP Filtering for Deep Learning Papers in Virology and Epidemiology\n",
    "### Abstract\n",
    "This study presents a targeted approach for filtering and classifying academic research papers from a dataset retrieved through keyword-based searches on PubMed, aimed at identifying works that apply deep learning neural network methods within virology and epidemiology. The dataset comprises 11,450 records collected with keywords related to machine learning and domain-specific terms. Using advanced Natural Language Processing (NLP) techniques, particularly the Sentence-BERT (SBERT) model, this analysis processes and refines the dataset to identify papers most relevant to neural network-based deep learning implementations. SBERT embeddings enable high-quality semantic representation of abstracts, facilitating efficient similarity-based filtering to measure and rank the relevance of each paper to specified topics. This methodology allows for the effective extraction of key research trends, techniques, and applications within virology and epidemiology, supporting researchers in quickly locating impactful studies on deep learning applications in these fields.\n",
    "\n",
    "### Task\n",
    "1. Implement semantic natural language processing techniques to filter out papers that do not meet\n",
    "the criteria of utilizing deep learning approaches in virology/epidemiology.\n",
    "2. For the papers deemed relevant, classify them according to the type of method used: [\"text\n",
    "mining\", \"computer vision\", \"both\", \"other\"].\n",
    "3. Extract and report the name of the method used for each relevant paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8795b02-1805-477a-b2d1-c18cdef770e1",
   "metadata": {},
   "source": [
    "### Libraries and Dataset Loading\n",
    "This section imports necessary libraries and loads the dataset containing abstracts of research papers on virology and epidemiology with an emphasis on machine learning applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "99bb3b22-1e7b-4240-ae3b-f193dab88d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import torch\n",
    "import scipy.stats as stats\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "from collections import Counter\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.SettingWithCopyWarning)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c146f37-e37a-4863-921e-eefec71bfe3c",
   "metadata": {},
   "source": [
    "### Data Collection Procedure\n",
    "\n",
    "The dataset for this study was sourced from the publicly accessible **Virology AI Papers Repository**, which compiles research articles at the intersection of virology, epidemiology, and artificial intelligence applications. The repository contains metadata of **11,450 academic papers** from PubMed, focusing on deep learning and neural networks in virology and epidemiology. The dataset includes fields such as PubMed ID (PMID), paper title, authors, citations, journal/book, publication year, DOI, and abstracts (optional). It provides accessible metadata for research and links to PubMed Central for full-text access when available.\n",
    "\n",
    "**Data Source Repository**: [Virology AI Papers Repository](https://github.com/jd-coderepos/virology-ai-papers/)\n",
    "\n",
    "**Data Source**: [PubMed](https://pubmed.ncbi.nlm.nih.gov/)\n",
    "\n",
    "###  Query\n",
    "\n",
    "1. **Neural Networks**: 3,791 results  \n",
    "   Query: (((virology) OR (epidemiology)) AND ((\"neural network\") OR (\"artificial neural network\") OR (\"machine learning model\") OR (\"feedforward neural network\") OR (\"neural net algorithm\") OR (\"multilayer perceptron\") OR (\"convolutional neural network\") OR (\"recurrent neural network\") OR (\"long short-term memory network\") OR (\"CNN\") OR (\"GRNN\") OR (\"RNN\") OR (\"LSTM\")))  \n",
    "\n",
    "\n",
    "2. **Deep Learning**: 2,664 results  \n",
    "   Query: (((virology) OR (epidemiology)) AND ((\"deep learning\") OR (\"deep neural networks\")))  \n",
    "\n",
    "\n",
    "3. **Computer Vision**: 3,647 results  \n",
    "   Query: (((virology) OR (epidemiology)) AND ((\"computer vision\") OR (\"vision model\") OR (\"image processing\") OR (\"vision algorithms\") OR (\"computer graphics and vision\") OR (\"object recognition\") OR (\"scene understanding\")))  \n",
    "\n",
    "\n",
    "4. **Natural Language Processing (NLP)**: 2,237 results  \n",
    "   Query: (((virology) OR (epidemiology)) AND ((\"natural language processing\") OR (\"text mining\") OR (NLP) OR (\"computational linguistics\") OR (\"language processing\") OR (\"text analytics\") OR (\"textual data analysis\") OR (\"text data analysis\") OR (\"text analysis\") OR (\"speech and language technology\") OR (\"language modeling\") OR (\"computational semantics\")))  \n",
    "\n",
    "\n",
    "5. **Generative AI**: 75 results  \n",
    "   Query: (((virology) OR (epidemiology)) AND ((\"generative artificial intelligence\") OR (\"generative AI\") OR (\"generative deep learning\") OR (\"generative models\")))  \n",
    "\n",
    "\n",
    "6. **Transformer Models**: 262 results  \n",
    "   Query: (((virology) OR (epidemiology)) AND ((\"transformer models\") OR (\"self-attention models\") OR (\"transformer architecture\") OR (\"transformer\") OR (\"attention-based neural networks\") OR (\"transformer networks\") OR (\"sequence-to-sequence models\")))  \n",
    "\n",
    "\n",
    "7. **Large Language Models (LLM)**: 183 results  \n",
    "   Query: (((virology) OR (epidemiology)) AND ((\"large language model\") OR (llm) OR (\"transformer-based model\") OR (\"pretrained language model\") OR (\"generative language model\") OR (\"foundation model\") OR (\"state-of-the-art language model\")))  \n",
    "\n",
    "\n",
    "8. **Multimodal Models**: 121 results  \n",
    "   Query: (((virology) OR (epidemiology)) AND ((\"multimodal model\") OR (\"multimodal neural network\") OR (\"vision transformer\") OR (\"diffusion model\") OR (\"generative diffusion model\") OR (\"diffusion-based generative model\") OR (\"continuous diffusion model\")))  \n",
    "\n",
    "---\n",
    "\n",
    "**Total records**: 12,980 (with duplicates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff3a94b-1c4c-4321-be2f-a00d1aaab75a",
   "metadata": {},
   "source": [
    "### Data Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c35efd9-4b26-437e-ae8e-b1f796b932a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PMID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Citation</th>\n",
       "      <th>First Author</th>\n",
       "      <th>Journal/Book</th>\n",
       "      <th>Publication Year</th>\n",
       "      <th>Create Date</th>\n",
       "      <th>PMCID</th>\n",
       "      <th>NIHMS ID</th>\n",
       "      <th>DOI</th>\n",
       "      <th>Abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39435445</td>\n",
       "      <td>Editorial: The operationalization of cognitive...</td>\n",
       "      <td>Winter M, Probst T, Tallon M, Schobel J, Pryss R.</td>\n",
       "      <td>Front Neurosci. 2024 Oct 7;18:1501636. doi: 10...</td>\n",
       "      <td>Winter M</td>\n",
       "      <td>Front Neurosci</td>\n",
       "      <td>2024</td>\n",
       "      <td>2024/10/22</td>\n",
       "      <td>PMC11491427</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.3389/fnins.2024.1501636</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39398866</td>\n",
       "      <td>Characterization of arteriosclerosis based on ...</td>\n",
       "      <td>Zhou J, Li X, Demeke D, Dinh TA, Yang Y, Janow...</td>\n",
       "      <td>J Med Imaging (Bellingham). 2024 Sep;11(5):057...</td>\n",
       "      <td>Zhou J</td>\n",
       "      <td>J Med Imaging (Bellingham)</td>\n",
       "      <td>2024</td>\n",
       "      <td>2024/10/14</td>\n",
       "      <td>PMC11466048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1117/1.JMI.11.5.057501</td>\n",
       "      <td>PURPOSE: Our purpose is to develop a computer ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39390053</td>\n",
       "      <td>Multi-scale input layers and dense decoder agg...</td>\n",
       "      <td>Lan X, Jin W.</td>\n",
       "      <td>Sci Rep. 2024 Oct 10;14(1):23729. doi: 10.1038...</td>\n",
       "      <td>Lan X</td>\n",
       "      <td>Sci Rep</td>\n",
       "      <td>2024</td>\n",
       "      <td>2024/10/10</td>\n",
       "      <td>PMC11467340</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1038/s41598-024-74701-0</td>\n",
       "      <td>Accurate segmentation of COVID-19 lesions from...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39367648</td>\n",
       "      <td>An initial game-theoretic assessment of enhanc...</td>\n",
       "      <td>Fatemi MY, Lu Y, Diallo AB, Srinivasan G, Azhe...</td>\n",
       "      <td>Brief Bioinform. 2024 Sep 23;25(6):bbae476. do...</td>\n",
       "      <td>Fatemi MY</td>\n",
       "      <td>Brief Bioinform</td>\n",
       "      <td>2024</td>\n",
       "      <td>2024/10/05</td>\n",
       "      <td>PMC11452536</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1093/bib/bbae476</td>\n",
       "      <td>The application of deep learning to spatial tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39363262</td>\n",
       "      <td>Truncated M13 phage for smart detection of E. ...</td>\n",
       "      <td>Yuan J, Zhu H, Li S, Thierry B, Yang CT, Zhang...</td>\n",
       "      <td>J Nanobiotechnology. 2024 Oct 3;22(1):599. doi...</td>\n",
       "      <td>Yuan J</td>\n",
       "      <td>J Nanobiotechnology</td>\n",
       "      <td>2024</td>\n",
       "      <td>2024/10/04</td>\n",
       "      <td>PMC11451008</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1186/s12951-024-02881-y</td>\n",
       "      <td>BACKGROUND: The urgent need for affordable and...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PMID                                              Title  \\\n",
       "0  39435445  Editorial: The operationalization of cognitive...   \n",
       "1  39398866  Characterization of arteriosclerosis based on ...   \n",
       "2  39390053  Multi-scale input layers and dense decoder agg...   \n",
       "3  39367648  An initial game-theoretic assessment of enhanc...   \n",
       "4  39363262  Truncated M13 phage for smart detection of E. ...   \n",
       "\n",
       "                                             Authors  \\\n",
       "0  Winter M, Probst T, Tallon M, Schobel J, Pryss R.   \n",
       "1  Zhou J, Li X, Demeke D, Dinh TA, Yang Y, Janow...   \n",
       "2                                      Lan X, Jin W.   \n",
       "3  Fatemi MY, Lu Y, Diallo AB, Srinivasan G, Azhe...   \n",
       "4  Yuan J, Zhu H, Li S, Thierry B, Yang CT, Zhang...   \n",
       "\n",
       "                                            Citation First Author  \\\n",
       "0  Front Neurosci. 2024 Oct 7;18:1501636. doi: 10...     Winter M   \n",
       "1  J Med Imaging (Bellingham). 2024 Sep;11(5):057...       Zhou J   \n",
       "2  Sci Rep. 2024 Oct 10;14(1):23729. doi: 10.1038...        Lan X   \n",
       "3  Brief Bioinform. 2024 Sep 23;25(6):bbae476. do...    Fatemi MY   \n",
       "4  J Nanobiotechnology. 2024 Oct 3;22(1):599. doi...       Yuan J   \n",
       "\n",
       "                 Journal/Book  Publication Year Create Date        PMCID  \\\n",
       "0              Front Neurosci              2024  2024/10/22  PMC11491427   \n",
       "1  J Med Imaging (Bellingham)              2024  2024/10/14  PMC11466048   \n",
       "2                     Sci Rep              2024  2024/10/10  PMC11467340   \n",
       "3             Brief Bioinform              2024  2024/10/05  PMC11452536   \n",
       "4         J Nanobiotechnology              2024  2024/10/04  PMC11451008   \n",
       "\n",
       "  NIHMS ID                         DOI  \\\n",
       "0      NaN  10.3389/fnins.2024.1501636   \n",
       "1      NaN   10.1117/1.JMI.11.5.057501   \n",
       "2      NaN  10.1038/s41598-024-74701-0   \n",
       "3      NaN         10.1093/bib/bbae476   \n",
       "4      NaN  10.1186/s12951-024-02881-y   \n",
       "\n",
       "                                            Abstract  \n",
       "0                                                NaN  \n",
       "1  PURPOSE: Our purpose is to develop a computer ...  \n",
       "2  Accurate segmentation of COVID-19 lesions from...  \n",
       "3  The application of deep learning to spatial tr...  \n",
       "4  BACKGROUND: The urgent need for affordable and...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = 'https://raw.githubusercontent.com/jd-coderepos/virology-ai-papers/main/collection_with_abstracts.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0111af-c30a-430d-b720-e8abad26afaa",
   "metadata": {},
   "source": [
    "### Data Description\n",
    "To understand the distribution of topics and keywords in the dataset, this section presents a series of visualizations on the frequency and relevance of keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ada01cb-ce29-44ee-a903-e335da46ae4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11450 entries, 0 to 11449\n",
      "Data columns (total 12 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   PMID              11450 non-null  int64 \n",
      " 1   Title             11450 non-null  object\n",
      " 2   Authors           11450 non-null  object\n",
      " 3   Citation          11450 non-null  object\n",
      " 4   First Author      11450 non-null  object\n",
      " 5   Journal/Book      11450 non-null  object\n",
      " 6   Publication Year  11450 non-null  int64 \n",
      " 7   Create Date       11450 non-null  object\n",
      " 8   PMCID             6450 non-null   object\n",
      " 9   NIHMS ID          956 non-null    object\n",
      " 10  DOI               10969 non-null  object\n",
      " 11  Abstract          11237 non-null  object\n",
      "dtypes: int64(2), object(10)\n",
      "memory usage: 1.0+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Data description process\n",
    "print(data.info())  # General information about the dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9aab172-6090-4756-abf2-07b6b9a69c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    11237.000000\n",
      "mean      1723.080182\n",
      "std        575.944666\n",
      "min          4.000000\n",
      "25%       1386.000000\n",
      "50%       1699.000000\n",
      "75%       1983.000000\n",
      "max      20492.000000\n",
      "Name: Abstract, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#  distribution of abstract lengths\n",
    "abstract_lengths = data['Abstract'].str.len()\n",
    "print(abstract_lengths.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26640554-079c-4784-9276-3ca10d02211c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing abstracts: 213, Initial size: 11450, Filtered size: 11237\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Count the number of missing abstracts\n",
    "missing_abstracts_count = data['Abstract'].isna().sum()\n",
    "\n",
    "# Filter out entries without abstracts\n",
    "data_with_abstract = data.dropna(subset=['Abstract'])\n",
    "\n",
    "# Report the dataset size before and after filtering\n",
    "initial_size = data.shape[0]\n",
    "filtered_size = data_with_abstract.shape[0]\n",
    "\n",
    "print(f\"Missing abstracts: {missing_abstracts_count}, Initial size: {initial_size}, Filtered size: {filtered_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "74c2d6e4-6a38-4250-a5df-c30e99874c95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"1020px\"\n",
       "    height=\"420\"\n",
       "    src=\"iframe_figures/figure_46.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Create subplots: 1 row, 2 columns\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=(\"Paper with Abstracts Availability\", \"Distribution of Abstract Lengths\"))\n",
    "\n",
    "# Plot 1: Paper Abstracts Availability\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=['Present', 'Missing'],\n",
    "        y=[filtered_size, missing_abstracts_count],\n",
    "        marker_color=['blue', 'red'],\n",
    "        text=[filtered_size, missing_abstracts_count],  \n",
    "        textposition='outside'  \n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Plot 2: Distribution of Abstract Lengths\n",
    "fig.add_trace(\n",
    "    go.Histogram(\n",
    "        x=abstract_lengths,  # Abstract lengths on the X-axis\n",
    "        marker_color='blue',\n",
    "        opacity=0.7,\n",
    "           \n",
    "       \n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Update layout for combined figure\n",
    "fig.update_layout(\n",
    "    showlegend=False,\n",
    "    hovermode='x unified',\n",
    "    height=400,\n",
    "    width=1000,  # Adjust width to give more space for histograms\n",
    "    template='plotly_dark',  # Apply dark theme for better contrast\n",
    "    xaxis=dict(\n",
    "        range=[0, max(abstract_lengths)]),\n",
    "    margin=dict(l=50, r=50, t=50, b=50)  # Adjust margins for readability\n",
    ")\n",
    "\n",
    "# X and Y-axis titles for each subplot\n",
    "fig.update_xaxes(title_text=\"Abstract Status\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Number of Papers\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Length of Abstract (characters)\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Number of Papers\", row=1, col=2)\n",
    "\n",
    "# Show the figure\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404d39e1-4792-455c-926a-9c756509dad2",
   "metadata": {},
   "source": [
    "**Data Preprocessing**\n",
    "\n",
    "The preprocessing of abstracts involves applying several NLP techniques to clean and prepare the text for further analysis. This includes converting the text to lowercase, removing special characters, lemmatizing words to their root forms, eliminating stopwords, and expanding certain terms to ensure they are relevant for deep learning models. These steps help standardize the text and improve its effectiveness in subsequent modeling tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c87e51b8-a19d-4397-8e94-1c183cbb9976",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\dsohi\\Documents\\Semantic NLP Filtering for\n",
      "[nltk_data]     Deep Learning Papers in Virology\n",
      "[nltk_data]     Epidemiology\\myenv\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dsohi\\Documents\\Semantic NLP Filtering for\n",
      "[nltk_data]     Deep Learning Papers in Virology\n",
      "[nltk_data]     Epidemiology\\myenv\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\dsohi\\Documents\\Semantic NLP Filtering for\n",
      "[nltk_data]     Deep Learning Papers in Virology\n",
      "[nltk_data]     Epidemiology\\myenv\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab') \n",
    "\n",
    "# Initialize lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Define a single preprocessing function\n",
    "def preprocess_abstracts(data, abstract_col='Abstract'):\n",
    "    \"\"\"\n",
    "    Preprocesses the abstracts by applying multiple NLP steps:\n",
    "    - Converts text to lowercase\n",
    "    - Removes special characters\n",
    "    - Lemmatizes words to base form\n",
    "    - Removes stopwords\n",
    " \n",
    "    \n",
    "    Parameters:\n",
    "        data (pd.DataFrame): DataFrame containing abstracts in the specified column\n",
    "        abstract_col (str): Column name containing abstracts (default 'Abstract')\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with preprocessed abstracts and sentence segmentation\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    def preprocess_text(text):\n",
    "        # Lowercase conversion\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove special characters and digits\n",
    "        text = re.sub(r'[^a-z\\s]', '', text)\n",
    "        \n",
    "        # Lemmatize and remove stopwords\n",
    "        text = ' '.join([lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words])\n",
    "        \n",
    "        return text\n",
    "    \n",
    "\n",
    "    \n",
    "    # Apply preprocessing steps with .loc to avoid SettingWithCopyWarning\n",
    "    data = data.copy()  # Ensure we're working with a full DataFrame copy to prevent chained assignment\n",
    "    data.loc[:, abstract_col] = data[abstract_col].astype(str)  # Ensure text format\n",
    "    data.loc[:, 'Preprocessed_Abstract'] = data[abstract_col].apply(preprocess_text)\n",
    "\n",
    "    \n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf1689ef-8139-4711-b946-ffc2bae7bba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_abstract = preprocess_abstracts(data_with_abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c046ca4-379b-4205-b57d-eee1dee78f86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    purpose purpose develop computer vision approa...\n",
       "2    accurate segmentation covid lesion medical ima...\n",
       "3    application deep learning spatial transcriptom...\n",
       "4    background urgent need affordable rapid detect...\n",
       "5    background incidental extrapulmonary finding c...\n",
       "Name: Preprocessed_Abstract, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_with_abstract['Preprocessed_Abstract'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1fe602-c6c5-49bb-8afb-042747fe935c",
   "metadata": {},
   "source": [
    "### TASK 1 : Implement semantic natural language processing techniques to filter out papers that do not meet the criteria of utilizing deep learning approaches in virology/epidemiology."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd829f8",
   "metadata": {},
   "source": [
    "**Semantic natural language processing technique using Senetence Transformers (SBERT)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "109aa403-8885-4acf-a963-243779709353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a lightweight transformer model suitable for efficient semantic embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # Optimal for faster processing on larger datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31df00ea",
   "metadata": {},
   "source": [
    "**Keywords related to Deep learning based on query search performed on original data collection process**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df71a28a-ed72-4921-b744-69a2294e0ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_learning_keywords = [\n",
    "    \n",
    "    # Neural Networks\n",
    "    \"neural network\", \"artificial neural network\", \"machine learning model\", \n",
    "    \"feedforward neural network\", \"neural net algorithm\", \"multilayer perceptron\", \n",
    "    \"convolutional neural network\", \"recurrent neural network\", \n",
    "    \"long short-term memory network\", \"CNN\", \"GRNN\", \"RNN\", \"LSTM\",\n",
    "    \n",
    "    # Deep Learning\n",
    "    \"deep learning\", \"deep neural networks\",\n",
    "    \n",
    "    # Computer Vision\n",
    "    \"computer vision\", \"vision model\", \"image processing\", \n",
    "    \"vision algorithms\", \"computer graphics and vision\", \"object recognition\", \n",
    "    \"scene understanding\",\n",
    "    \n",
    "    # Natural Language Processing\n",
    "    \"natural language processing\", \"text mining\", \"NLP\", \n",
    "    \"computational linguistics\", \"language processing\", \"text analytics\", \n",
    "    \"textual data analysis\", \"speech and language technology\", \n",
    "    \"language modeling\", \"computational semantics\",\n",
    "    \n",
    "    # Generative AI\n",
    "    \"generative artificial intelligence\", \"generative AI\", \n",
    "    \"generative deep learning\", \"generative models\",\n",
    "    \n",
    "    # Transformer Models\n",
    "    \"transformer models\", \"self-attention models\", \"transformer architecture\", \n",
    "    \"transformer\", \"attention-based neural networks\", \"transformer networks\", \n",
    "    \"sequence-to-sequence models\",\n",
    "    \n",
    "    # Large Language Models\n",
    "    \"large language model\", \"LLM\", \"transformer-based model\", \n",
    "    \"pretrained language model\", \"generative language model\", \n",
    "    \"foundation model\", \"state-of-the-art language model\",\n",
    "    \n",
    "    # Multimodal Models\n",
    "    \"multimodal model\", \"multimodal neural network\", \n",
    "    \"vision transformer\", \"diffusion model\", \n",
    "    \"generative diffusion model\", \"diffusion-based generative model\", \n",
    "    \"continuous diffusion model\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c9677d",
   "metadata": {},
   "source": [
    "**Embeddings for the abstracts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ae74fc8-6386-4c1c-bb70-069bb91444d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# data_with_abstract['Abstract_Embeddings'] = data_with_abstract['Preprocessed_Abstract'].apply(lambda x: model.encode(x, convert_to_tensor=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e188cfd6",
   "metadata": {},
   "source": [
    "Created a Pickle and csv  file to save the dataframe after preprocessed embedding and for faster processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "763e9881",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_with_abstract.to_pickle('results/Preprocessed_dataset.pkl')\n",
    "data_with_abstract.to_csv('results/Preprocessed_dataset.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a787c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_abstract = pd.read_pickle('data/Preprocessed_dataset.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e68c4ab",
   "metadata": {},
   "source": [
    "**Embeddings for the keywords**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb980fab-32dc-4876-b619-0e66bbf8e345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 359 ms\n",
      "Wall time: 237 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "keyword_embeddings = model.encode(deep_learning_keywords, convert_to_tensor=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee1abf2",
   "metadata": {},
   "source": [
    "**Function to calculate similarity percentage between defined deep learning keywords and abstract of each paper**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c9d39e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentage_similarity(abstract_embedding, keyword_embeddings):\n",
    "    \"\"\"\n",
    "    Calculates a percentage similarity score between the abstract embedding and keyword embeddings.\n",
    "\n",
    "    Parameters:\n",
    "    - abstract_embedding: The embedding vector of the abstract.\n",
    "    - keyword_embeddings: The embedding matrix of keywords.\n",
    "\n",
    "    Returns:\n",
    "    - float: Percentage similarity score between 0% and 100%.\n",
    "    \"\"\"\n",
    "    # Compute cosine similarities between the abstract and keyword embeddings\n",
    "    similarities = util.pytorch_cos_sim(abstract_embedding, keyword_embeddings)\n",
    "    \n",
    "    # Get the maximum similarity score\n",
    "    max_similarity = similarities.max().item()\n",
    "    \n",
    "    # Convert cosine similarity to percentage (mapping from [-1, 1] to [0%, 100%])\n",
    "    percentage_similarity = ((max_similarity + 1) / 2) * 100\n",
    "    \n",
    "    return percentage_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4dd06ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  percentage_similarity function\n",
    "data_with_abstract['Deep_Learning_similarity_percentage'] = data_with_abstract['Abstract_Embeddings'].apply(\n",
    "    lambda x: percentage_similarity(x, keyword_embeddings)  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70f2dc6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Deep_Learning_similarity_percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Characterization of arteriosclerosis based on ...</td>\n",
       "      <td>64.648607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Multi-scale input layers and dense decoder agg...</td>\n",
       "      <td>67.409289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>An initial game-theoretic assessment of enhanc...</td>\n",
       "      <td>67.685950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Truncated M13 phage for smart detection of E. ...</td>\n",
       "      <td>60.238410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AI for Multistructure Incidental Findings and ...</td>\n",
       "      <td>58.529333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11445</th>\n",
       "      <td>The characteristics of epidemics and invasions...</td>\n",
       "      <td>66.230896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11446</th>\n",
       "      <td>Effects of sales promotion on smoking among U....</td>\n",
       "      <td>64.551564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11447</th>\n",
       "      <td>Hypertension in an inner-city minority population</td>\n",
       "      <td>68.643083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11448</th>\n",
       "      <td>Aerosol transmission of a viable virus affecti...</td>\n",
       "      <td>74.284615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11449</th>\n",
       "      <td>Role of desolvation energy in the nonfacilitat...</td>\n",
       "      <td>63.360879</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11237 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Title  \\\n",
       "1      Characterization of arteriosclerosis based on ...   \n",
       "2      Multi-scale input layers and dense decoder agg...   \n",
       "3      An initial game-theoretic assessment of enhanc...   \n",
       "4      Truncated M13 phage for smart detection of E. ...   \n",
       "5      AI for Multistructure Incidental Findings and ...   \n",
       "...                                                  ...   \n",
       "11445  The characteristics of epidemics and invasions...   \n",
       "11446  Effects of sales promotion on smoking among U....   \n",
       "11447  Hypertension in an inner-city minority population   \n",
       "11448  Aerosol transmission of a viable virus affecti...   \n",
       "11449  Role of desolvation energy in the nonfacilitat...   \n",
       "\n",
       "       Deep_Learning_similarity_percentage  \n",
       "1                                64.648607  \n",
       "2                                67.409289  \n",
       "3                                67.685950  \n",
       "4                                60.238410  \n",
       "5                                58.529333  \n",
       "...                                    ...  \n",
       "11445                            66.230896  \n",
       "11446                            64.551564  \n",
       "11447                            68.643083  \n",
       "11448                            74.284615  \n",
       "11449                            63.360879  \n",
       "\n",
       "[11237 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_with_abstract[['Title','Deep_Learning_similarity_percentage']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac4211a",
   "metadata": {},
   "source": [
    "**Histogram to Visulaise the distribution of similarity percentage across the abstracts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "710e4981",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_47.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Calculate the mean and standard deviation\n",
    "mean = np.mean(data_with_abstract['Deep_Learning_similarity_percentage'])\n",
    "std_dev = np.std(data_with_abstract['Deep_Learning_similarity_percentage'])\n",
    "\n",
    "# Create the histogram plot\n",
    "fig = px.histogram(\n",
    "    data_with_abstract, \n",
    "    x='Deep_Learning_similarity_percentage',  \n",
    "    nbins=30,  \n",
    "    color_discrete_sequence=['#1f77b4'],  \n",
    "    title='Distribution of Similarity Percentages',  \n",
    "    labels={'Deep_Learning_similarity_percentage': 'Similarity Percentage', 'count': 'Number of Papers'}  \n",
    ")\n",
    "\n",
    "# Generate the normal distribution curve\n",
    "x_values = np.linspace(min(data_with_abstract['Deep_Learning_similarity_percentage']), \n",
    "                       max(data_with_abstract['Deep_Learning_similarity_percentage']), 100)\n",
    "y_values = stats.norm.pdf(x_values, mean, std_dev) * len(data_with_abstract) * (x_values[1] - x_values[0])\n",
    "\n",
    "# Add the normal distribution curve to the histogram\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=x_values, \n",
    "    y=y_values, \n",
    "    mode='lines', \n",
    "    name='Normal Distribution', \n",
    "    line=dict(color='red', width=2)\n",
    "))\n",
    "\n",
    "# Shade the significant area (e.g., 95% of the distribution)\n",
    "x_fill = np.linspace(mean - 1.96*std_dev, mean + 1.96*std_dev, 100)\n",
    "y_fill = stats.norm.pdf(x_fill, mean, std_dev) * len(data_with_abstract) * (x_values[1] - x_values[0])\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=np.concatenate([x_fill, x_fill[::-1]]), \n",
    "    y=np.concatenate([y_fill, np.zeros_like(y_fill)]),\n",
    "    fill='toself',\n",
    "    fillcolor='rgba(0, 255, 0, 0.3)',  \n",
    "    line=dict(color='green', width=0),\n",
    "    name='Significant Area (95%)'\n",
    "))\n",
    "\n",
    "# Add an annotation on the left end (for example, near the mean - 1.96 * std_dev)\n",
    "fig.add_annotation(\n",
    "    x=mean - 2*std_dev,  # Position the annotation at two standard deviations left of the mean\n",
    "    y=0.05,  # Adjust the y position for better visibility\n",
    "    text=\"Significant Area (Lower End)\",\n",
    "    showarrow=True,\n",
    "    arrowhead=2,\n",
    "    ax=30,  # Adjust the arrow size\n",
    "    ay=-40,  # Adjust the arrow size\n",
    "    font=dict(size=12, color=\"white\"),\n",
    "    align=\"center\",\n",
    "    arrowcolor=\"white\"\n",
    ")\n",
    "\n",
    "# Update layout with customized styling and font sizes\n",
    "fig.update_layout(\n",
    "    title_font_size=18,  \n",
    "    xaxis_title_font_size=16,  \n",
    "    yaxis_title_font_size=16,  \n",
    "    template='plotly_dark',  \n",
    "    xaxis=dict(showgrid=True, zeroline=False),  \n",
    "    yaxis=dict(showgrid=True, zeroline=False),  \n",
    "    bargap=0.2  \n",
    ")\n",
    "\n",
    "# Display the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6ae77e",
   "metadata": {},
   "source": [
    "**based on the visual interpreation similarity percentage above 55 are considered as Deep learning apporaches**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b43e6c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total papers found: 11237\n",
      "Total relevant papers similarity with deep learning approaches: 11040\n"
     ]
    }
   ],
   "source": [
    "filtered_papers = data_with_abstract[data_with_abstract['Deep_Learning_similarity_percentage'] >mean - 2*std_dev]\n",
    "print(f\"Total papers found: {len(data_with_abstract)}\")\n",
    "print(f\"Total relevant papers similarity with deep learning approaches: {len(filtered_papers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "073b37a5-3d48-4b3b-9132-77c823ae0790",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_abstract[['Title','Abstract','Deep_Learning_similarity_percentage']].to_csv('results/Task1_Deep_learning_papers.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1242dc2",
   "metadata": {},
   "source": [
    "**Comparison with Keyword based appoach** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a8ddf113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total papers with at least one keyword match: 6924\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Convert the keywords list to lowercase for case-insensitive matching\n",
    "deep_learning_keywords_lower = [kw.lower() for kw in deep_learning_keywords]\n",
    "\n",
    "data_with_abstract_keyword = data_with_abstract.copy()\n",
    "\n",
    "# Function to check keyword occurrence in the abstract\n",
    "def keyword_match_count(abstract):\n",
    "    abstract_lower = abstract.lower()  # Convert abstract to lowercase\n",
    "    count = sum(kw in abstract_lower for kw in deep_learning_keywords_lower)  # Count keyword occurrences\n",
    "    return count\n",
    "\n",
    "\n",
    "data_with_abstract_keyword['Keyword_Match_Count'] = data_with_abstract_keyword['Abstract'].apply(keyword_match_count)\n",
    "\n",
    "#Filter paper with at least one count \n",
    "keyword_filtered_papers = data_with_abstract_keyword[data_with_abstract_keyword['Keyword_Match_Count'] > 0]\n",
    "\n",
    "# Reporting the relevant papers\n",
    "print(f\"Total papers with at least one keyword match: {len(keyword_filtered_papers)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c2732516-036f-4d35-a2ae-9a9f4db32be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Keyword_Match_Count  Count\n",
      "0                    0   4313\n",
      "1                    1   3736\n",
      "2                    2   1884\n",
      "3                    3    933\n",
      "4                    4    260\n",
      "5                    5     66\n",
      "6                    6     31\n",
      "7                    8     10\n",
      "8                    7      4\n"
     ]
    }
   ],
   "source": [
    "# Count occurrences of each Keyword_Match_Count value\n",
    "keyword_count = data_with_abstract_keyword['Keyword_Match_Count'].value_counts().reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "keyword_count.columns = ['Keyword_Match_Count', 'Count']\n",
    "\n",
    "# Display the result\n",
    "print(keyword_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "383e5bc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_24.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = px.histogram(\n",
    "    data_with_abstract_keyword, \n",
    "    x='Keyword_Match_Count',  # Column with keyword match counts\n",
    "    nbins=20,  # Set number of bins\n",
    "    color_discrete_sequence=['#1f77b4'],  # Set the color to a blue shade\n",
    "    title='Distribution of Keyword Match Count',  # Plot title\n",
    "    labels={'Keyword_Match_Count': 'Keyword Match Count', 'count': 'No of Papers'}  # Axis labels\n",
    ")\n",
    "\n",
    "# Update layout with better styling\n",
    "fig.update_layout(\n",
    "    title_font_size=18,  # Increase title font size\n",
    "    xaxis_title_font_size=16,  # Increase X-axis label font size\n",
    "    yaxis_title_font_size=16,  # Increase Y-axis label font size\n",
    "    template='plotly_dark',  # Apply dark theme\n",
    "    xaxis=dict(showgrid=True, zeroline=False),  # Display gridlines\n",
    "    yaxis=dict(showgrid=True, zeroline=False),  # Remove zero-line for cleaner look\n",
    "    bargap=0.2,  # Adjust the bar gap for better spacing\n",
    "    hovermode='x unified'  # Show hover info across the bars\n",
    ")\n",
    "\n",
    "# Display the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ceddc04-c01c-4a08-8498-4da07c815359",
   "metadata": {},
   "source": [
    "**Results Summary:** <br> \n",
    "\n",
    "the SBERT approach identifies a significant number of relevant papers, with a total of 11,237 papers found and 11,040 of them showing relevance to deep learning methods. In contrast, the keyword-based approach shows a less refined classification, with 4,313 papers having no matches, and fewer papers being classified with higher counts of keyword matches, such as 66 papers with 5 keyword matches. This demonstrates SBERT's superior ability to detect deep learning-related papers by capturing more nuanced contextual information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b0fd5e-442c-4afc-93d9-ae22ed660335",
   "metadata": {},
   "source": [
    "### TASK 2 : For the papers deemed relevant, classify them according to the type of method used: [\"text mining\", \"computer vision\", \"both\", \"other\"]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b4b33b-8efe-4844-b7fa-498b38717165",
   "metadata": {},
   "source": [
    "**Method categories** <br>\n",
    "Keywords have been categorized into two groups: Text Mining and Computer Vision, based on the terms used in the original data collection query. These categories were determined by identifying specific keywords related to text mining techniques (such as NLP and computational linguistics) and computer vision methods (such as image processing and object recognition) used to filter the relevant academic papers in virology and epidemiology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "06da27cb-5c08-4343-bef9-b3bf1fde015f",
   "metadata": {},
   "outputs": [],
   "source": [
    "method_categories = {\n",
    "    \"text mining\": [\n",
    "        \"natural language processing\", \"text mining\", \"NLP\", \"computational linguistics\", \"language processing\",\n",
    "            \"text analytics\", \"textual data analysis\", \"speech and language technology\", \"language modeling\",\n",
    "            \"computational semantics\" ],\n",
    "    \"computer vision\": [\n",
    "            \"computer vision\", \"vision model\", \"image processing\", \"vision algorithms\", \"computer graphics and vision\",\n",
    "            \"object recognition\", \"scene understanding\" ],\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4228e5a1-85f9-40ce-9b59-39201426c85c",
   "metadata": {},
   "source": [
    "**Embeddings for the keywords**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "58ad35e3-c390-4129-b02e-cc3c3b560b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "method_embeddings = {}\n",
    "for method, keywords in method_categories.items():\n",
    "    method_embeddings[method] = model.encode(keywords, convert_to_tensor=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0337a8-e551-4bce-a7d3-aa74daad07f2",
   "metadata": {},
   "source": [
    "**Function for Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "54d10445-7d98-48a9-afd5-0d06ee09950b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_methods(abstract_embedding, threshold=0.2):\n",
    "    \"\"\"\n",
    "    Classify an abstract into methods based on its embedding similarity to predefined method embeddings.\n",
    "    \n",
    "    Parameters:\n",
    "    abstract_embedding (tensor): The embedding of the abstract to be classified.\n",
    "    threshold (float): The minimum cosine similarity score for classification.\n",
    "    \n",
    "    Returns:\n",
    "    str: The classification of the abstract (either 'Both', 'text mining', 'computer vision', or 'other').\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize a dictionary to store similarity scores for each method\n",
    "    similarities = {}\n",
    "    \n",
    "    # Calculate cosine similarity for each method (text mining, computer vision)\n",
    "    for method, embeddings in method_embeddings.items():\n",
    "        similarities[method] = util.pytorch_cos_sim(abstract_embedding, embeddings).max().item()\n",
    "\n",
    "    # Check if both methods exceed the threshold\n",
    "    if similarities['text mining'] > threshold and similarities['computer vision'] > threshold:\n",
    "        return \"Both\"\n",
    "\n",
    "    # Check if only text mining method exceeds the threshold\n",
    "    elif similarities['text mining'] > threshold:\n",
    "        return \"text mining\"\n",
    "    \n",
    "    # Check if only computer vision method exceeds the threshold\n",
    "    elif similarities['computer vision'] > threshold:\n",
    "        return \"computer vision\"\n",
    " \n",
    "    # Return \"other\" if neither method exceeds the threshold\n",
    "    else:\n",
    "        return \"other\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "123388ed-33fb-421f-ade0-773e58753bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method_Type\n",
      "other              3751\n",
      "text mining        3085\n",
      "Both               2502\n",
      "computer vision    1702\n",
      "Name: count, dtype: int64\n",
      "CPU times: total: 3.16 s\n",
      "Wall time: 3.78 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "filtered_papers['Method_Type']   = filtered_papers['Abstract_Embeddings'].apply(classify_methods)\n",
    "\n",
    "# Display the counts of each method type\n",
    "method_type_counts = filtered_papers['Method_Type'].value_counts()\n",
    "print(method_type_counts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c64359e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_29.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Get the counts of each method type\n",
    "method_type_counts = filtered_papers['Method_Type'].value_counts()\n",
    "\n",
    "# Create a bar plot with improved style and theme\n",
    "fig = go.Figure(data=[go.Bar(\n",
    "    x=method_type_counts.index,  # Method types as x-axis labels\n",
    "    y=method_type_counts.values,  # Number of papers as y-axis values\n",
    "    text=method_type_counts.values,  # Text to display on top of each bar\n",
    "    marker=dict(color='dodgerblue', line=dict(color='black', width=1))  # Custom bar color with border\n",
    ")])\n",
    "\n",
    "# Update layout with enhanced styling\n",
    "fig.update_layout(\n",
    "    title='Number of Papers and  Category Classification (SBERT Approach)',\n",
    "    title_font_size=18,  # Title font size\n",
    "    title_x=0.5,  # Center the title\n",
    "    xaxis_title='Method Type',\n",
    "    yaxis_title='Number of Papers',\n",
    "    xaxis_tickangle=-45,  # Angle the x-axis labels for better readability\n",
    "    template='plotly_dark',  # Dark theme for high contrast\n",
    "    showlegend=False,  # Hide legend as it's unnecessary here\n",
    "    plot_bgcolor='#1E1E1E',  # Background color for the plot area\n",
    "    paper_bgcolor='#2E2E2E',  # Background color for the paper area\n",
    "\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "821bc3bd-51e4-4140-844c-01b6dba140c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_papers[['Title','Method_Type']].to_csv('results/Task2_Classification_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db00cb52-a183-43ff-abc1-722efdbfcdb1",
   "metadata": {},
   "source": [
    "**Comparison with Keyword based appoach** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7171fd79-41f5-4ddb-ad2b-c264e34f6e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category\n",
      "other              9300\n",
      "text mining        1485\n",
      "computer vision     447\n",
      "both                  5\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Convert the method categories keywords to lowercase for case-insensitive matching\n",
    "method_categories_lower = {\n",
    "    category: [kw.lower() for kw in keywords]\n",
    "    for category, keywords in method_categories.items()\n",
    "}\n",
    "\n",
    "# Function to classify the paper based on its abstract\n",
    "def classify_based_on_keyword(abstract):\n",
    "    abstract_lower = abstract.lower()  # Convert the abstract to lowercase\n",
    "    \n",
    "    # Track categories that have matching keywords\n",
    "    matched_categories = []\n",
    "\n",
    "    # Check if any keywords match for 'text mining'\n",
    "    if any(keyword in abstract_lower for keyword in method_categories_lower[\"text mining\"]):\n",
    "        matched_categories.append(\"text mining\")\n",
    "    \n",
    "    # Check if any keywords match for 'computer vision'\n",
    "    if any(keyword in abstract_lower for keyword in method_categories_lower[\"computer vision\"]):\n",
    "        matched_categories.append(\"computer vision\")\n",
    "    \n",
    "    # Classification logic based on the number of matched categories\n",
    "    if \"text mining\" in matched_categories and \"computer vision\" in matched_categories:\n",
    "        return \"both\"\n",
    "    elif \"text mining\" in matched_categories:\n",
    "        return \"text mining\"\n",
    "    elif \"computer vision\" in matched_categories:\n",
    "        return \"computer vision\"\n",
    "    else:\n",
    "        return \"other\"  # If no match for either category\n",
    "\n",
    "\n",
    "# Apply the classification function to each paper's abstract\n",
    "data_with_abstract_keyword['Category'] = data_with_abstract_keyword['Abstract'].apply(classify_based_on_keyword)\n",
    "\n",
    "# Get the count of papers in each category\n",
    "category_counts = data_with_abstract_keyword['Category'].value_counts()\n",
    "\n",
    "# Print the category counts\n",
    "print(category_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b19e34b5-3768-4847-ba36-936174ace379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_32.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the counts of each method type\n",
    "method_type_counts = data_with_abstract_keyword['Category'].value_counts()\n",
    "\n",
    "# Create a bar plot with improved style and theme\n",
    "fig = go.Figure(data=[go.Bar(\n",
    "    x=method_type_counts.index,  # Method types as x-axis labels\n",
    "    y=method_type_counts.values,  # Number of papers as y-axis values\n",
    "    text=method_type_counts.values,  # Text to display on top of each bar\n",
    "    marker=dict(color='dodgerblue', line=dict(color='black', width=1))  # Custom bar color with border\n",
    ")])\n",
    "\n",
    "# Update layout with enhanced styling\n",
    "fig.update_layout(\n",
    "    title='Number of Papers and Category Classification (Keyword Search Approach)',\n",
    "    title_font_size=18,  # Title font size\n",
    "    title_x=0.5,  # Center the title\n",
    "    xaxis_title='Method Type',\n",
    "    yaxis_title='Number of Papers',\n",
    "    xaxis_tickangle=-45,  # Angle the x-axis labels for better readability\n",
    "    template='plotly_dark',  # Dark theme for high contrast\n",
    "    showlegend=False,  # Hide legend as it's unnecessary here\n",
    "    plot_bgcolor='#1E1E1E',  # Background color for the plot area\n",
    "    paper_bgcolor='#2E2E2E',  # Background color for the paper area\n",
    "    margin=dict(l=50, r=50, t=50, b=50)  # Adjust margins for readability\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badce513-44c8-49aa-b734-6353651d6a40",
   "metadata": {},
   "source": [
    "**Results Summary:**<br> \n",
    "The SBERT-based classification method outperforms the keyword-based approach in several key areas. While SBERT provides more accurate categorizations, especially in identifying papers that involve both text mining and computer vision (2502 papers classified as \"Both\"), the keyword-based method struggles to classify mixed-method papers, reporting only 5. Additionally, SBERT avoids overclassifying papers into the \"Other\" category, unlike the keyword method, which categorizes 9300 papers as \"Other\". This highlights SBERT's superior ability to capture contextual and hybrid method usage in papers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9459413c-6ddc-4d24-acfc-4dc6fc13bd44",
   "metadata": {},
   "source": [
    "### TASK 3 : Extract and report the name of the method used for each relevant paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26858feb-3b2a-4151-bec7-01a027e8ab84",
   "metadata": {},
   "source": [
    "**Consolidated Keywords set used in data collection query**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1c41422d-c5cd-4932-ac09-cb8060ddd84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Apporaches = [\n",
    "        \"feedforward neural network\", \"neural net algorithm\", \"multilayer perceptron\", \n",
    "        \"convolutional neural network\", \n",
    "        \"recurrent neural network\", \"long short-term memory network\", \"CNN\", \"GRNN\", \n",
    "        \"RNN\", \"LSTM\", \"computer vision\", \"vision model\", \"image processing\", \"vision algorithms\", \n",
    "        \"computer graphics and vision\", \"object recognition\", \"scene understanding\",\n",
    "        \"natural language processing\", \"text mining\", \"NLP\", \"computational linguistics\", \n",
    "        \"language processing\", \"text analytics\", \"textual data analysis\", \n",
    "        \"speech and language technology\", \"language modeling\", \"computational semantics\",\n",
    "        \"generative artificial intelligence\", \"generative AI\", \"generative deep learning\", \n",
    "        \"generative models\",\n",
    "        \"transformer models\", \"self-attention models\", \"transformer architecture\", \n",
    "        \"transformer\", \"attention-based neural networks\", \"transformer networks\", \n",
    "        \"sequence-to-sequence models\",\n",
    "        \"large language model\", \"LLM\", \"transformer-based model\", \"pretrained language model\", \n",
    "        \"generative language model\", \"foundation model\", \"state-of-the-art language model\",\n",
    "        \"multimodal model\", \"multimodal neural network\", \"vision transformer\", \"diffusion model\", \n",
    "        \"generative diffusion model\", \"diffusion-based generative model\", \"continuous diffusion model\"\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02845316-a734-4bb0-8973-77a4a3d3b1e5",
   "metadata": {},
   "source": [
    "**Embeddings for the keywords**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b36f1102-09e6-47d6-8c1a-4e952829f859",
   "metadata": {},
   "outputs": [],
   "source": [
    " Apporaches_embedding = model.encode(Apporaches, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ef562d4a-ebe9-4305-a5d9-234e4e35c40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to identify the method with the highest similarity for each abstract\n",
    "def extract_methods(abstract_embedding, approaches_embeddings, approaches):\n",
    "    \"\"\"\n",
    "    This function calculates the cosine similarities between the abstract's embedding \n",
    "    and a list of approach embeddings, then returns the approach with the highest similarity.\n",
    "    \n",
    "    Parameters:\n",
    "        abstract_embedding (tensor): The embedding of the paper's abstract.\n",
    "        approaches_embeddings (tensor): Embeddings for various approaches to compare with the abstract.\n",
    "        approaches (list): List of method names corresponding to the embeddings.\n",
    "        \n",
    "    Returns:\n",
    "        best_match (str): The method name with the highest similarity to the abstract.\n",
    "    \"\"\"\n",
    "    # Compute cosine similarities between the abstract and each approach's embedding\n",
    "    cosine_similarities = util.cos_sim(abstract_embedding, approaches_embeddings)\n",
    "    \n",
    "    # Find the index of the approach with the highest similarity score\n",
    "    best_match_idx = torch.argmax(cosine_similarities).item()\n",
    "    \n",
    "    # Get the approach name and the similarity value for the best match\n",
    "    best_match = approaches[best_match_idx]\n",
    "    best_similarity = cosine_similarities[0][best_match_idx].item()\n",
    "    \n",
    "    # Return the approach with the highest similarity\n",
    "    return best_match\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "357befb5-f43d-423b-9084-34362ed4ebba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to each abstract in the dataset\n",
    "filtered_papers['Method_used'] = filtered_papers['Abstract_Embeddings'].apply(\n",
    "    lambda embedding: extract_methods(embedding, Apporaches_embedding, Apporaches)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "78f26271-f438-42d9-9c87-0249b653e1dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Method_used</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Characterization of arteriosclerosis based on ...</td>\n",
       "      <td>computer graphics and vision</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Multi-scale input layers and dense decoder agg...</td>\n",
       "      <td>image processing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>An initial game-theoretic assessment of enhanc...</td>\n",
       "      <td>RNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Truncated M13 phage for smart detection of E. ...</td>\n",
       "      <td>feedforward neural network</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AI for Multistructure Incidental Findings and ...</td>\n",
       "      <td>text mining</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11445</th>\n",
       "      <td>The characteristics of epidemics and invasions...</td>\n",
       "      <td>multimodal model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11446</th>\n",
       "      <td>Effects of sales promotion on smoking among U....</td>\n",
       "      <td>continuous diffusion model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11447</th>\n",
       "      <td>Hypertension in an inner-city minority population</td>\n",
       "      <td>LSTM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11448</th>\n",
       "      <td>Aerosol transmission of a viable virus affecti...</td>\n",
       "      <td>diffusion model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11449</th>\n",
       "      <td>Role of desolvation energy in the nonfacilitat...</td>\n",
       "      <td>diffusion model</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11040 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Title  \\\n",
       "1      Characterization of arteriosclerosis based on ...   \n",
       "2      Multi-scale input layers and dense decoder agg...   \n",
       "3      An initial game-theoretic assessment of enhanc...   \n",
       "4      Truncated M13 phage for smart detection of E. ...   \n",
       "5      AI for Multistructure Incidental Findings and ...   \n",
       "...                                                  ...   \n",
       "11445  The characteristics of epidemics and invasions...   \n",
       "11446  Effects of sales promotion on smoking among U....   \n",
       "11447  Hypertension in an inner-city minority population   \n",
       "11448  Aerosol transmission of a viable virus affecti...   \n",
       "11449  Role of desolvation energy in the nonfacilitat...   \n",
       "\n",
       "                        Method_used  \n",
       "1      computer graphics and vision  \n",
       "2                  image processing  \n",
       "3                               RNN  \n",
       "4        feedforward neural network  \n",
       "5                       text mining  \n",
       "...                             ...  \n",
       "11445              multimodal model  \n",
       "11446    continuous diffusion model  \n",
       "11447                          LSTM  \n",
       "11448               diffusion model  \n",
       "11449               diffusion model  \n",
       "\n",
       "[11040 rows x 2 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = filtered_papers[['Title','Method_used']]\n",
    "filtered_papers[['Title','Method_used']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3002291f-bd56-4f4c-8bdb-bae79fd5008b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Method_used\n",
       "text analytics                        1195\n",
       "feedforward neural network             960\n",
       "multimodal neural network              935\n",
       "pretrained language model              756\n",
       "natural language processing            747\n",
       "RNN                                    713\n",
       "text mining                            650\n",
       "vision model                           624\n",
       "LSTM                                   498\n",
       "image processing                       448\n",
       "vision transformer                     311\n",
       "CNN                                    305\n",
       "sequence-to-sequence models            273\n",
       "neural net algorithm                   266\n",
       "long short-term memory network         233\n",
       "convolutional neural network           200\n",
       "textual data analysis                  184\n",
       "recurrent neural network               183\n",
       "generative deep learning               160\n",
       "vision algorithms                      130\n",
       "computer graphics and vision           128\n",
       "computer vision                        122\n",
       "language modeling                      101\n",
       "generative AI                          100\n",
       "multimodal model                       100\n",
       "foundation model                        87\n",
       "NLP                                     78\n",
       "object recognition                      61\n",
       "generative artificial intelligence      57\n",
       "continuous diffusion model              46\n",
       "speech and language technology          45\n",
       "transformer models                      43\n",
       "language processing                     42\n",
       "diffusion-based generative model        34\n",
       "computational semantics                 33\n",
       "generative models                       29\n",
       "self-attention models                   26\n",
       "large language model                    24\n",
       "attention-based neural networks         19\n",
       "diffusion model                         18\n",
       "transformer                             13\n",
       "LLM                                     13\n",
       "transformer networks                    11\n",
       "transformer-based model                 10\n",
       "generative diffusion model               7\n",
       "generative language model                6\n",
       "transformer architecture                 6\n",
       "multilayer perceptron                    5\n",
       "GRNN                                     3\n",
       "computational linguistics                2\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Get the counts of each method used in the filtered papers\n",
    "method_counts = filtered_papers['Method_used'].value_counts()\n",
    "method_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2464fdd9-a0b8-4bbf-8520-9dd6c8e7aeb2",
   "metadata": {},
   "source": [
    "**Top 10 Methods Used in Papers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "764b57f6-a576-4877-a0fe-b22a075c390a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_39.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Flatten the list of methods used in the 'Method_used' column and count frequencies\n",
    "all_methods = [method for method in filtered_papers['Method_used']]\n",
    "\n",
    "# Count the frequency of each method using Counter\n",
    "method_counts = Counter(all_methods)\n",
    "\n",
    "# Get the top 10 most common methods\n",
    "top_10_methods = method_counts.most_common(10)\n",
    "\n",
    "# Unzip the method and count data for plotting\n",
    "methods, counts = zip(*top_10_methods)\n",
    "\n",
    "# Create a Plotly bar chart with frequency labels on top of the bars\n",
    "fig = go.Figure(data=[go.Bar(\n",
    "    x=methods, \n",
    "    y=counts, \n",
    "    text=counts,  # Add frequency labels\n",
    "    marker=dict(\n",
    "        color=counts, \n",
    "        colorscale='Viridis',  # More visually appealing color scale\n",
    "        line=dict(color='black', width=1)  # Add border around bars\n",
    "    )\n",
    ")])\n",
    "\n",
    "# Update layout with enhanced styling\n",
    "fig.update_layout(\n",
    "    title='Top 10 Methods Used in Papers',\n",
    "    title_font_size=18,  # Increase title font size\n",
    "    title_x=0.5,  # Center the title\n",
    "    xaxis_title=\"Method\",\n",
    "    yaxis_title=\"Frequency\",\n",
    "    template='plotly_dark',  # Dark theme for high contrast\n",
    "    xaxis_tickangle=45,  # Angle x-axis labels for better readability\n",
    "    showlegend=False,  # Hide legend as it's unnecessary here\n",
    "    plot_bgcolor='#1E1E1E',  # Background color for the plot area\n",
    "    paper_bgcolor='#2E2E2E',  # Background color for the paper area\n",
    "    margin=dict(l=50, r=50, t=50, b=50)  # Adjust margins for readability\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e64ce83-47d1-459f-a2a5-18825fa54218",
   "metadata": {},
   "source": [
    "**Tree map tto visulaise method used and count of papers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f479a3c0-6bbf-43d6-9e17-e4eca2a7fd5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"1020px\"\n",
       "    height=\"820\"\n",
       "    src=\"iframe_figures/figure_48.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the counts of each method used in the filtered papers\n",
    "method_counts = filtered_papers['Method_used'].value_counts()\n",
    "\n",
    "# Create a DataFrame with the method names (approaches) and their counts\n",
    "data = pd.DataFrame({\n",
    "    \"Approach\": method_counts.index,\n",
    "    \"Count\": method_counts.values\n",
    "})\n",
    "\n",
    "# Create the treemap using Plotly\n",
    "fig = px.treemap(data, \n",
    "                 path=[\"Approach\"],  # Single-level path (approach)\n",
    "                 values=\"Count\",  # The count for each approach\n",
    "                 title=\"Approach Distribution in Papers\",\n",
    "                 color=\"Count\",  # Color based on the count of each method\n",
    "                 color_continuous_scale=\"Viridis\")\n",
    "\n",
    "fig.update_layout(\n",
    "    width=1000,  # Width of the plot\n",
    "    height=800,  # Height of the plot\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "05261434-a71b-4c67-88ce-91515d5df235",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('results/Task3_method_identification_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "98205441-bdfb-4ca8-8ac1-3b5e100396c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_papers.to_csv('results/Dataset_with_complete_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636c4399-c46d-40a8-9026-42927b173408",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1d55f481-a947-4de6-9b8b-36e7719986a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip freeze > requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a794dda3-6e88-4a02-9c02-8f959420b38c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
